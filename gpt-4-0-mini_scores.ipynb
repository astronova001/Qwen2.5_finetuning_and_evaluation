{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from openai import AzureOpenAI  # Azure OpenAI client\n",
    "from tqdm import tqdm  # For progress bars\n",
    "import argparse  # For command-line arguments\n",
    "\n",
    "\n",
    "# Download NLTK data if not present\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "def initialize_azure_openai_client():\n",
    "    \"\"\"Initialize the Azure OpenAI client.\"\"\"\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=\"https://nora-south-india.openai.azure.com/\",  # Replace with your Azure endpoint\n",
    "        api_key=\"API_KEY\",  # Replace with your API key\n",
    "        api_version=\"2024-02-01\"\n",
    "    )\n",
    "    print(\"Azure OpenAI Client Initialized\")\n",
    "    return client\n",
    "\n",
    "# Normalize text for fair comparison\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text for fair comparison.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return str(text).lower().strip()\n",
    "\n",
    "# Extract JSON content from generated text\n",
    "def extract_json_from_text(text):\n",
    "    \"\"\"Extract JSON content from generated text, handling multi-line JSON structures.\"\"\"\n",
    "    try:\n",
    "        # Find all possible JSON patterns in text\n",
    "        json_pattern = r'\\{(?:[^{}]|(?:\\{.*\\}))*\\}'  \n",
    "        matches = re.findall(json_pattern, text, re.DOTALL)\n",
    "        \n",
    "        if matches:\n",
    "            for match in matches[::-1]:  # Check from the last occurrence\n",
    "                try:\n",
    "                    return json.loads(match)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting JSON: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Calculate BLEU and ROUGE scores\n",
    "def calculate_metrics(generated_text, reference_text):\n",
    "    \"\"\"Calculate BLEU and ROUGE scores between generated and reference texts.\"\"\"\n",
    "    try:\n",
    "        # Normalize texts\n",
    "        gen_normalized = normalize_text(generated_text)\n",
    "        ref_normalized = normalize_text(reference_text)\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        rouge_scores = scorer.score(ref_normalized, gen_normalized)\n",
    "        \n",
    "        # Prepare for BLEU calculation\n",
    "        reference_tokens = [nltk.word_tokenize(ref_normalized)]\n",
    "        generated_tokens = nltk.word_tokenize(gen_normalized)\n",
    "        \n",
    "        # Calculate BLEU with smoothing\n",
    "        smooth = SmoothingFunction()\n",
    "        bleu1 = sentence_bleu(reference_tokens, generated_tokens, \n",
    "                             weights=(1, 0, 0, 0),\n",
    "                             smoothing_function=smooth.method1)\n",
    "        \n",
    "        bleu4 = sentence_bleu(reference_tokens, generated_tokens, \n",
    "                             weights=(0.25, 0.25, 0.25, 0.25),\n",
    "                             smoothing_function=smooth.method1)\n",
    "        \n",
    "        return {\n",
    "            'bleu1': bleu1,\n",
    "            'bleu4': bleu4,\n",
    "            'rouge1': rouge_scores['rouge1'].fmeasure,\n",
    "            'rouge2': rouge_scores['rouge2'].fmeasure, \n",
    "            'rougeL': rouge_scores['rougeL'].fmeasure\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating metrics: {e}\")\n",
    "        return {\n",
    "            'bleu1': 0.0, 'bleu4': 0.0,\n",
    "            'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0\n",
    "        }\n",
    "\n",
    "\n",
    "# Calculate exact match rate for JSON objects\n",
    "def calculate_exact_match_rate(generated_json, reference_json):\n",
    "    \"\"\"Calculate exact match rate for JSON objects.\"\"\"\n",
    "    try:\n",
    "        if not generated_json or not reference_json:\n",
    "            return 0.0\n",
    "            \n",
    "        # Extract missing points arrays\n",
    "        gen_points = generated_json.get('pointsMissed', [])\n",
    "        ref_points = reference_json.get('pointsMissed', [])\n",
    "        \n",
    "        if not gen_points or not ref_points:\n",
    "            return 0.0\n",
    "            \n",
    "        # Normalize points\n",
    "        gen_points = [normalize_text(p) for p in gen_points]\n",
    "        ref_points = [normalize_text(p) for p in ref_points]\n",
    "        \n",
    "        # Calculate how many points match exactly\n",
    "        matches = sum(1 for p in gen_points if p in ref_points)\n",
    "        \n",
    "        # Calculate precision: matches / generated points\n",
    "        precision = matches / len(gen_points) if gen_points else 0\n",
    "        \n",
    "        # Calculate recall: matches / reference points\n",
    "        recall = matches / len(ref_points) if ref_points else 0\n",
    "        \n",
    "        # F1 score\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating exact match: {e}\")\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "\n",
    "# Process evaluation data\n",
    "total_metrics = {\n",
    "    'bleu1': [], 'bleu4': [],\n",
    "    'rouge1': [], 'rouge2': [], 'rougeL': [],\n",
    "    'precision': [], 'recall': [], 'f1': []\n",
    "}\n",
    "\n",
    "# Generate a response using Azure OpenAI GPT-4\n",
    "def generate_with_azure_openai(client, prompt, max_retries=3):\n",
    "    \"\"\"Generate a response using Azure OpenAI GPT-4 with retries.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",  # Replace with your deployment name\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0,\n",
    "                max_tokens=300,\n",
    "                top_p=0,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0\n",
    "            )\n",
    "            if response.choices and response.choices[0].message.content:\n",
    "                return response.choices[0].message.content.strip()\n",
    "            else:\n",
    "                print(f\"Attempt {attempt + 1}: Empty response from Azure OpenAI.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1}: Error generating with Azure OpenAI: {e}\")\n",
    "        time.sleep(2)  # Wait before retrying\n",
    "    return \"\"\n",
    "\n",
    "# Load data from JSON file\n",
    "with open('new_updated_examples.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Initialize Azure OpenAI client with your API key\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=\"https://nora-south-india.openai.azure.com/\",\n",
    "    api_key=API_KEY,  # Using the API_KEY variable defined earlier\n",
    "    api_version=\"2024-02-01\"\n",
    ")\n",
    "print(\"Azure OpenAI Client Initialized\")\n",
    "\n",
    "for i in data:\n",
    "    try:\n",
    "        mes = '''\n",
    "Ultra-Strict Expert Evaluation Rules:youare gove a answer to be covered json list blocks you have tand the answrr cointains the points related to points to be covered you are suppoed to select which points list blocks is missing you have to select only one list block ss the aswer and nothing feom other list blocks and by any case do not mix the asnwers \n",
    "don't do like this -\n",
    "reference_output -->{\"pointsMissed\": [\"All is well\", \"Ending the conversation\"]}\n",
    "Generated outut --> Points missed: [['All is well', 'Informal response', 'Ending the conversation']]\n",
    "this is wrong the generated output should be  [\"All is well\", \"Ending the conversation\"] nothing else even if u get the answers wrong its fine but don't mixup the solution  \n",
    "''' \n",
    "\n",
    "        ans = mes + i['conversations'][1]['content']\n",
    "        reference_output = i['conversations'][2]['content']\n",
    "        message = ans\n",
    "        print('\\nEvaluating example:')\n",
    "        # print('Reference output:', reference_output)\n",
    "        generated_text = generate_with_azure_openai(client, ans)\n",
    "        # print(f'Generated outut --> {generated_text}')\n",
    "        reference_json = extract_json_from_text(reference_output)\n",
    "        generated_json = extract_json_from_text(generated_text)\n",
    "        print(f'reference_json: {reference_json}')\n",
    "        print(f'Generated_json: {generated_json}')\n",
    "        # Calculate text-level metrics\n",
    "        text_metrics = calculate_metrics(generated_text, reference_output)\n",
    "        print(\"Text Metrics:\", text_metrics)\n",
    "        \n",
    "        # Calculate JSON-level metrics\n",
    "        json_metrics = calculate_exact_match_rate(generated_json, reference_json)\n",
    "        print(\"JSON Metrics:\", json_metrics)\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        for key in text_metrics:\n",
    "            total_metrics[key].append(text_metrics[key])\n",
    "        for key in json_metrics:\n",
    "            total_metrics[key].append(json_metrics[key])\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example: {e}\")\n",
    "        continue\n",
    "\n",
    "# Calculate and display average metrics\n",
    "print(\"\\n===== EVALUATION RESULTS =====\")\n",
    "print(\"\\nAverage Metrics:\")\n",
    "for key in total_metrics:\n",
    "    if total_metrics[key]:\n",
    "        avg = np.mean(total_metrics[key])\n",
    "        print(f\"{key}: {avg:.4f}\")\n",
    "\n",
    "print(\"\\nNumber of examples evaluated:\", len(total_metrics['bleu1']))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
