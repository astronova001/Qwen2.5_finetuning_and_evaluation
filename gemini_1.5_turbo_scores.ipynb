{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import google.generativeai as genai\n",
    "from tqdm import tqdm  # For progress bars\n",
    "import argparse  # For command-line arguments\n",
    "\n",
    "# Download NLTK data if not present\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Initialize Gemini model\n",
    "def initialize_gemini_model():\n",
    "    \"\"\"Initialize the Gemini model.\"\"\"\n",
    "    model_name = 'gemini-1.5-flash'\n",
    "    api_key = 'API_KEY'  # Replace with your actual API key\n",
    "    if not api_key:\n",
    "        raise ValueError(\"GEMINI_API_KEY environment variable is not set.\")\n",
    "    \n",
    "    genai.configure(api_key=api_key)\n",
    "    print(\"Gemini Model Initialized\")\n",
    "    return genai.GenerativeModel(model_name)\n",
    "\n",
    "# Normalize text for fair comparison\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize text for fair comparison.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    return str(text).lower().strip()\n",
    "\n",
    "# Extract JSON content from generated text\n",
    "def extract_json_from_text(text):\n",
    "    \"\"\"Extract JSON content from generated text.\"\"\"\n",
    "    try:\n",
    "        # Find JSON pattern in the text\n",
    "        json_pattern = r'\\{[^{}]*\\}'\n",
    "        matches = re.findall(json_pattern, text)\n",
    "        if matches:\n",
    "            # Try to parse each match until valid JSON is found\n",
    "            for match in reversed(matches):\n",
    "                try:\n",
    "                    return json.loads(match)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting JSON: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Calculate BLEU and ROUGE scores\n",
    "def calculate_metrics(generated_text, reference_text):\n",
    "    \"\"\"Calculate BLEU and ROUGE scores between generated and reference texts.\"\"\"\n",
    "    try:\n",
    "        # Normalize texts\n",
    "        gen_normalized = normalize_text(generated_text)\n",
    "        ref_normalized = normalize_text(reference_text)\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        rouge_scores = scorer.score(ref_normalized, gen_normalized)\n",
    "        \n",
    "        # Prepare for BLEU calculation\n",
    "        reference_tokens = [nltk.word_tokenize(ref_normalized)]\n",
    "        generated_tokens = nltk.word_tokenize(gen_normalized)\n",
    "        \n",
    "        # Calculate BLEU with smoothing\n",
    "        smooth = SmoothingFunction()\n",
    "        bleu1 = sentence_bleu(reference_tokens, generated_tokens, \n",
    "                             weights=(1, 0, 0, 0),\n",
    "                             smoothing_function=smooth.method1)\n",
    "        \n",
    "        bleu4 = sentence_bleu(reference_tokens, generated_tokens, \n",
    "                             weights=(0.25, 0.25, 0.25, 0.25),\n",
    "                             smoothing_function=smooth.method1)\n",
    "        \n",
    "        return {\n",
    "            'bleu1': bleu1,\n",
    "            'bleu4': bleu4,\n",
    "            'rouge1': rouge_scores['rouge1'].fmeasure,\n",
    "            'rouge2': rouge_scores['rouge2'].fmeasure, \n",
    "            'rougeL': rouge_scores['rougeL'].fmeasure\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating metrics: {e}\")\n",
    "        return {\n",
    "            'bleu1': 0.0, 'bleu4': 0.0,\n",
    "            'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0\n",
    "        }\n",
    "\n",
    "# Calculate exact match rate for JSON objects\n",
    "def calculate_exact_match_rate(generated_json, reference_json):\n",
    "    \"\"\"Calculate exact match rate for JSON objects.\"\"\"\n",
    "    try:\n",
    "        if not generated_json or not reference_json:\n",
    "            return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "            \n",
    "        # Extract missing points arrays\n",
    "        gen_points = generated_json.get('pointsMissed', [])\n",
    "        ref_points = reference_json.get('pointsMissed', [])\n",
    "        \n",
    "        # Flatten and normalize points\n",
    "        def flatten_and_normalize(points):\n",
    "            flat_list = []\n",
    "            for item in points:\n",
    "                if isinstance(item, list):\n",
    "                    flat_list.extend(flatten_and_normalize(item))\n",
    "                else:\n",
    "                    flat_list.append(normalize_text(item))\n",
    "            return flat_list\n",
    "        \n",
    "        gen_points = flatten_and_normalize(gen_points)\n",
    "        ref_points = flatten_and_normalize(ref_points)\n",
    "        \n",
    "        # Calculate how many points match exactly\n",
    "        matches = sum(1 for p in gen_points if p in ref_points)\n",
    "        \n",
    "        # Calculate precision, recall, and F1\n",
    "        precision = matches / len(gen_points) if gen_points else 0\n",
    "        recall = matches / len(ref_points) if ref_points else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating exact match: {e}\")\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "\n",
    "# Generate a response using the Gemini model\n",
    "def generate_with_gemini(model, prompt, max_retries=3):\n",
    "    \"\"\"Generate a response using the Gemini model with retries.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            if response.text:\n",
    "                return response.text\n",
    "            else:\n",
    "                print(f\"Attempt {attempt + 1}: Empty response from Gemini.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1}: Error generating with Gemini: {e}\")\n",
    "        time.sleep(2)  # Wait before retrying\n",
    "    return \"\"\n",
    "\n",
    "# Evaluate using Gemini model\n",
    "def evaluate_with_gemini(data, model=None):\n",
    "    \"\"\"Evaluate using Gemini model instead of the previous model.\"\"\"\n",
    "    # Initialize the Gemini model if not provided\n",
    "    if model is None:\n",
    "        model = initialize_gemini_model()\n",
    "    \n",
    "    # Process evaluation data\n",
    "    total_metrics = {\n",
    "        'bleu1': [], 'bleu4': [],\n",
    "        'rouge1': [], 'rouge2': [], 'rougeL': [],\n",
    "        'precision': [], 'recall': [], 'f1': []\n",
    "    }\n",
    "    \n",
    "    for idx, item in tqdm(enumerate(data), total=len(data), desc=\"Evaluating\"):\n",
    "        try:\n",
    "            # Prepare the prompt\n",
    "            mes = '''\n",
    "Ultra-Strict Expert Evaluation Rules:youare gove a answer to be covered json list blocks you have tand the answrr cointains the points related to points to be covered you are suppoed to select which points list blocks is missing you have to select only one list block ss the aswer and nothing feom other list blocks and by any case do not mix the asnwers from different lists under any s\n",
    "\n",
    "Example-\n",
    "answer- P1 p3 p4 p5\n",
    "list of points to be covered - {[p1,p2,p3],[p4,p5],[p6,p7]}\n",
    "points missed - {[p6,p7]}\n",
    "\n",
    "don't do like this -\n",
    "reference_output -->{\"pointsMissed\": [\"All is well\", \"Ending the conversation\"]}\n",
    "Generated outut --> Points missed: [['All is well', 'Informal response', 'Ending the conversation']]\n",
    "this is wrong the generated output should be  [\"All is well\", \"Ending the conversation\"] nothing else even if u get the answers wrong its fine but don't mixup the solution  \n",
    "''' \n",
    "\n",
    "            ans = mes + item['conversations'][1]['content']\n",
    "            reference_output = item['conversations'][2]['content']\n",
    "            \n",
    "            print(f'reference Text --> {reference_output}')\n",
    "            # Generate with Gemini\n",
    "            generated_text = generate_with_gemini(model, ans)\n",
    "            print(f'Generated_text --> {generated_text}')\n",
    "            # Extract JSON from both texts\n",
    "            reference_json = extract_json_from_text(reference_output)\n",
    "            generated_json = extract_json_from_text(generated_text)\n",
    "            \n",
    "            # Calculate text-level metrics\n",
    "            text_metrics = calculate_metrics(generated_text, reference_output)\n",
    "            \n",
    "            # Calculate JSON-level metrics\n",
    "            json_metrics = calculate_exact_match_rate(generated_json, reference_json)\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            for key in text_metrics:\n",
    "                total_metrics[key].append(text_metrics[key])\n",
    "            for key in json_metrics:\n",
    "                total_metrics[key].append(json_metrics[key])\n",
    "                \n",
    "            # Add a small delay to avoid rate limiting\n",
    "            time.sleep(1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {idx + 1}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Calculate and display average metrics\n",
    "    print(\"\\n===== EVALUATION RESULTS =====\")\n",
    "    print(\"\\nAverage Metrics:\")\n",
    "    for key in total_metrics:\n",
    "        if total_metrics[key]:\n",
    "            avg = np.mean(total_metrics[key])\n",
    "            print(f\"{key}: {avg:.4f}\")\n",
    "    \n",
    "    print(\"\\nNumber of examples evaluated:\", len(total_metrics['bleu1']))\n",
    "    \n",
    "    return total_metrics\n",
    "\n",
    "# Load data in batches\n",
    "def load_data_in_batches(data, batch_size):\n",
    "    \"\"\"Yield data in batches.\"\"\"\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size]\n",
    "\n",
    "# Batch evaluation\n",
    "def batch_evaluate_data():\n",
    "    \"\"\"Load data in batches and evaluate to manage memory.\"\"\"\n",
    "    # Load data from JSON file\n",
    "    with open('new_updated_examples.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Initialize Gemini model\n",
    "    model = initialize_gemini_model()\n",
    "    \n",
    "    # Process in batches\n",
    "    batch_size = 10\n",
    "    all_metrics = {\n",
    "        'bleu1': [], 'bleu4': [],\n",
    "        'rouge1': [], 'rouge2': [], 'rougeL': [],\n",
    "        'precision': [], 'recall': [], 'f1': []\n",
    "    }\n",
    "    \n",
    "    for batch in load_data_in_batches(data, batch_size):\n",
    "        print(f\"\\nProcessing batch...\")\n",
    "        \n",
    "        # Evaluate batch\n",
    "        batch_metrics = evaluate_with_gemini(batch, model)\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        for key in batch_metrics:\n",
    "            all_metrics[key].extend(batch_metrics[key])\n",
    "    \n",
    "    # Final aggregate results\n",
    "    print(\"\\n===== FINAL EVALUATION RESULTS =====\")\n",
    "    print(\"\\nOverall Average Metrics:\")\n",
    "    for key in all_metrics:\n",
    "        if all_metrics[key]:\n",
    "            avg = np.mean(all_metrics[key])\n",
    "            print(f\"{key}: {avg:.4f}\")\n",
    "    \n",
    "    print(\"\\nTotal examples evaluated:\", len(all_metrics['bleu1']))\n",
    "    \n",
    "    return all_metrics\n",
    "\n",
    "# Evaluate a single prompt-response pair\n",
    "def evaluate_single_pair():\n",
    "    \"\"\"Evaluate a single prompt-response pair for quick testing.\"\"\"\n",
    "    # Initialize Gemini model\n",
    "    model = initialize_gemini_model()\n",
    "    \n",
    "    # Test prompt\n",
    "    prompt = \"\"\"Act like an expert evaluator, where given an answer and a list of points to be covered in the answer you give a json of missing points\n",
    "Answer: Yeah. So the cost of product is $5. \n",
    "List of points to be covered: [['the cost of the product is 5', 'the product cost is 5', 'cost is 5 ', 'its cost is 5 ', 'just 5 ']]\"\"\"\n",
    "    \n",
    "    # Expected reference output\n",
    "    reference = \"\"\"{\"pointsMissed\": [\"the cost of the product is 5\", \"the product cost is 5\", \"cost is 5 \", \"its cost is 5 \", \"just 5 \"]}\"\"\"\n",
    "    \n",
    "    # Generate with Gemini\n",
    "    generated = generate_with_gemini(model, prompt)\n",
    "    print(\"\\nPrompt:\", prompt)\n",
    "    print(\"\\nReference Output:\", reference)\n",
    "    print(\"\\nGenerated Output:\", generated)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    text_metrics = calculate_metrics(generated, reference)\n",
    "    \n",
    "    # Extract JSON\n",
    "    reference_json = extract_json_from_text(reference)\n",
    "    generated_json = extract_json_from_text(generated)\n",
    "    json_metrics = calculate_exact_match_rate(generated_json, reference_json)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n===== SINGLE PAIR EVALUATION RESULTS =====\")\n",
    "    print(\"\\nText Metrics:\")\n",
    "    for key, value in text_metrics.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nJSON Content Metrics:\")\n",
    "    for key, value in json_metrics.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    return {**text_metrics, **json_metrics}\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Evaluate Gemini model.\")\n",
    "    parser.add_argument('--mode', choices=['single', 'batch'], default='batch', help=\"Evaluation mode: single or batch.\")\n",
    "    args, unknown = parser.parse_known_args()  # Ignore unrecognized arguments\n",
    "    \n",
    "    if args.mode == 'single':\n",
    "        evaluate_single_pair()\n",
    "    else:\n",
    "        batch_evaluate_data()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
