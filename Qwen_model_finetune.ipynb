{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "# !pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
    "\n",
    "from unsloth.chat_templates import get_chat_template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaned _data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_path = '/kaggle/working/cleaned_data_no_text.json'\n",
    "\n",
    "dataset = json.load(open(data_path))\n",
    "dataset[0]\n",
    "\n",
    "\n",
    "'''Example of dataset\n",
    "{'conversations': [{'content': 'You are an expert evaluator.',\n",
    "   'role': 'system'},\n",
    "  {'content': \"Act like an expert evaluator, where given an answer and a list of points to be covered in the answer you give a json of missing points.\\nAnswer:  . . . .  all don't know . . .  either of don't no. I don't. No. I don't. No. I don't. No. I don't don't. No. . Don't no. . Don't no. I don't. Know. Don't Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. Yes. We provide two schools. We provide two schools. B per y. Two schools. Wider two school, sweep wider two schools. Bp peer peer peer peer peer  blah blah blah. Testing interesting. . . I don't I don't so don't \\nList of points to be covered: [['Funding Schools', 'Supporting Education', 'Financial Assistance for Education', 'Investing in Education', 'Providing Resources to Schools'], ['Funding schools', 'Investing in education', 'Backing educational institutions', 'Granting financial aid', 'Supporting the youth'], ['Financial aid to schools', 'Supporting educational institutions', 'Boosting school budgets', 'Providing resources for education', 'Helping students succeed'], ['Offering monetary assistance to educational institutions', 'Assisting educational institutions financially', 'Supplying financial resources to schools', 'Providing economic support to educational establishments'], ['Providing financial support to schools', 'Helping educational institutions with funds', 'Assisting educational institutions with money', 'Allocating resources to school systems', 'Offering aid to school districts', 'Securing funds for education']]\",\n",
    "   'role': 'user'},\n",
    "  {'content': '{\"pointsMissed\": [\"Funding schools\", \"Investing in education\", \"Backing educational institutions\", \"Granting financial aid\", \"Supporting the youth\"]}',\n",
    "   'role': 'assistant'}]}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "data_path = '/kaggle/working/cleaned_data_no_text.json'\n",
    "\n",
    "# Load dataset\n",
    "dataset = json.load(open(data_path))\n",
    "\n",
    "# Different variations of system prompts\n",
    "system_prompt_variations = [\n",
    "    \"Analyze user responses and identify missing key points from the provided list. Return only the missing points in JSON format.\",\n",
    "    \"Ensure that the missing points match exactly as they appear in the 'Points to be Covered' section, without modification or paraphrasing.\",\n",
    "    \"Your task is to compare the user's response with a predefined list of key points. If any points are missing, return them as a structured JSON output, preserving their original wording.\",\n",
    "    \"You are an AI evaluation assistant. Given a response and a set of required points, identify and output the missing ones in JSON format.\",\n",
    "    \"Your expertise is needed to review responses and check if they cover all important aspects. Provide a structured JSON listing the missing points, ensuring they are copied verbatim from the original 'Points to be Covered' list.\"\n",
    "]\n",
    "\n",
    "# Modify system messages\n",
    "for item in dataset:\n",
    "    for convo in item['conversations']:\n",
    "        if convo['role'] == 'system':\n",
    "            convo['content'] = random.choice(system_prompt_variations)  # Randomly pick a variation\n",
    "\n",
    "# Save the modified dataset\n",
    "modified_data_path = '/kaggle/working/modified_cleaned_data.json'\n",
    "json.dump(dataset, open(modified_data_path, 'w'), indent=4)\n",
    "\n",
    "print(\"Dataset updated and saved to\", modified_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_path = '/kaggle/working/modified_cleaned_data.json'\n",
    "\n",
    "dataset = json.load(open(data_path))\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load dataset\n",
    "data_path = '/kaggle/working/modified_cleaned_data.json'  # Update path if needed\n",
    "with open(data_path) as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# Convert dataset to new structured ChatML format\n",
    "chatml_data = []\n",
    "preprompt = (\n",
    "    \"You are an expert evaluator. Compare the user’s answer with the given key points and return only the missing ones in JSON format. Ensure the missing points match exactly as they appear in the 'Points to be Covered' section, without modification or paraphrasing \"\n",
    ")\n",
    "\n",
    "for i, item in enumerate(dataset):\n",
    "    if \"conversations\" not in item:\n",
    "        print(f\"⚠️ Warning: Missing 'conversations' key in item {i}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    # Extract conversation parts\n",
    "    user_content = \"\"\n",
    "    expected_output = \"\"\n",
    "\n",
    "    for msg in item[\"conversations\"]:\n",
    "        if msg[\"role\"] == \"user\":\n",
    "            user_content = msg[\"content\"]\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            expected_output = msg[\"content\"]\n",
    "\n",
    "    # Format conversation using the template\n",
    "    chatml_conversation = (\n",
    "        f\"<|im_start|>system\\n{preprompt}<|endoftext|>\\n\"\n",
    "        f\"<|im_start|>user\\n{user_content}<|endoftext|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n{expected_output}<|endoftext|>\\n\"\n",
    "    )\n",
    "\n",
    "    chatml_data.append(chatml_conversation.strip())\n",
    "\n",
    "# Save the formatted dataset in JSONL\n",
    "output_path = \"chatml_template_format_1.jsonl\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    for chat in chatml_data:\n",
    "        f.write(json.dumps({\"chat\": chat}) + \"\\n\")\n",
    "\n",
    "print(f\"✅ Conversion completed! Saved as {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSONL file\n",
    "pat = \"chatml_template_format_1.jsonl\"\n",
    "with open(pat, \"r\") as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Function to format ChatML-style text\n",
    "def format_chatml(example):\n",
    "    if \"chat\" in example:  # Check for 'chat' key\n",
    "        return {\"text\": example[\"chat\"]}\n",
    "    else:\n",
    "        return {\"text\": \"\"}  # Handle missing cases\n",
    "\n",
    "# Apply formatting using list comprehension\n",
    "formatted_dataset = [format_chatml(example) for example in dataset]\n",
    "\n",
    "# Print the first formatted example for verification\n",
    "print(formatted_dataset[:1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert the list into a Hugging Face Dataset\n",
    "dataset = Dataset.from_dict({\"text\": [d[\"text\"] for d in formatted_dataset]})  \n",
    "\n",
    "print(dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch \n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True \n",
    "\n",
    "model,tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2.5-7B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Test code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# Manually format chat history into a single string\n",
    "fformatted_input = \"\"\"Act like an expert evaluator, where given an answer and a list of points to be covered in the answer you give a JSON of missing points.\n",
    "Answer: all don’t know ...\n",
    "List of points to be covered: [['Funding Schools', 'Supporting Education', 'Financial Assistance for Education', 'Investing in Education', 'Providing Resources to Schools'], \n",
    "['Funding schools', 'Investing in education', 'Backing educational institutions', 'Granting financial aid', 'Supporting the youth']]\"\"\"\n",
    "\n",
    "\n",
    "inputs = tokenizer(\n",
    "    formatted_input,\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")  # Convert to tensor and move to GPU if available\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# Initialize the streamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "# Generate response\n",
    "_ = model.generate(input_ids=inputs[\"input_ids\"], streamer=text_streamer, max_new_tokens=300,\n",
    "                   use_cache=True, temperature=0.7, min_p=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoraAdapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 64, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initilize the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# Define the formatting function\n",
    "def formatting_func(example):\n",
    "    return [example[\"chat\"]]  # Adjust this based on your dataset structure\n",
    "\n",
    "# Update SFTTrainer with formatting_func\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Temlate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|im_start|>user\\n\",\n",
    "    response_part = \"<|im_start|>assistant\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test decode \n",
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Define source and destination paths\n",
    "source_folder = \"/kaggle/working/Qwen2.5_lora_v1\"\n",
    "zip_path = \"/kaggle/working/Qwen2.5_lora_v1.zip\"\n",
    "\n",
    "# Create a zip file\n",
    "shutil.make_archive(zip_path.replace(\".zip\", \"\"), 'zip', source_folder)\n",
    "\n",
    "print(\"Zipping completed! File saved at:\", zip_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the finetuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "    # mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 200, use_cache = True,\n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from transformers import TextStreamer\n",
    "\n",
    "messages = [\n",
    "            {\n",
    "                \"content\": \"You are an expert evaluator.\",\n",
    "                \"role\": \"system\"\n",
    "            },\n",
    "            {\n",
    "                \"content\": \"\\nAnswer: Using educational policies to improve student outcomes is an important strategy for any educational system. And minimum viable products, Mvp, can play a role in this process by helping policymakers makers, educators, and state make informed decisions. Here are several reasons why Mvp should be employed for this purpose. Data driven decision making, Mvp allowed policymakers to gather real world data on the effectiveness of different educational policies and interventions. By analyzing this data, They can make informed decisions about which policies are most effective in improving student outcomes, cost effectiveness, implementing large scale educational policies can be expensive and time consuming. Mvp provide a cost effective way to test \\nList of points to be covered: [['Taking time off to relax', 'Letting go of stress and worries', 'Finding ways to unwind', \\\"Exploring Trovex's offerings\\\"], [\\\"Don't know about Trovex\\\", 'Relevant to Trovex', 'Summary and evaluation of any other answer'], ['Trovex  Innovative Storage Solutions', 'High-Quality, Durable Materials', 'Fully Customizable to Your Needs', 'Unrivaled Strength and Hygiene', 'Industry-Leading Installation Services']]\",\n",
    "                \"role\": \"user\"\n",
    "            },\n",
    "            {\n",
    "                \"content\": \"{\\\"pointsMissed\\\": [\\\"Taking time off to relax\\\", \\\"Letting go of stress and worries\\\", \\\"Finding ways to unwind\\\", \\\"Exploring Trovex's offerings\\\"]}\",\n",
    "                \"role\": \"assistant\"\n",
    "            }\n",
    "        ]\n",
    "messages_2 = [\n",
    "  {\n",
    "    \"content\": \"Act like an expert evaluator, where given an answer and a list of points to be covered in the answer you give a json of missing points.\",\n",
    "    \"role\": \"system\"\n",
    "  },\n",
    "  {\n",
    "    \"content\": \"Act like an expert evaluator, where given an answer and a list of points to be covered in the answer you give a json of missing points. Act like an expert evaluator, where given an answer and a list of points to be covered in the answer you give a json of missing points.\\nAnswer: Any free, open source JavaScript front-end library for building user interfaces to components and maintained by and collectively individual or mobile developers or companies using J.S.\\nList of points to be covered: [['\\\"Free library\\\"', 'UI development.', '\\\"Facebook\\\\'s creation\\\"'], ['Node js', 'Language for web development', 'JavaScript programming language ']]\",\n",
    "    \"role\": \"user\"\n",
    "  },\n",
    "  {\n",
    "    \"content\": \"{\\\"pointsMissed\\\": [\\\"Node js\\\"]}\",\n",
    "    \"role\": \"assistant\"\n",
    "  }\n",
    "]\n",
    "\n",
    "for i in data:\n",
    "    mes = 'Act like an expert evaluator, where given an answer and a list of points to be covered in the answer you give a json of missing points'\n",
    "    ans = mes + i['conversations'][1]['content']\n",
    "    points = i['conversations'][2]['content']\n",
    "    message =   ans\n",
    "    print('points Mised= ',points)\n",
    "    \n",
    "\n",
    "    \n",
    "# mes = 'Act like an expert evaluator, where given an answer and a list of points to be covered in the answer you give a json of missing points'\n",
    "# mid = mes + messages[1]['content'] \n",
    "# print(mid)\n",
    "# points_mised = messages[2]['content']\n",
    "# print('points Mised= ',points_mised)\n",
    "# # print('/n')\n",
    "# message =   mid\n",
    "\n",
    "# # testing Place\n",
    "\n",
    "    # Tokenize input and move to GPU\n",
    "    inputs = tokenizer(message, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Initialize the streamer\n",
    "    text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "    \n",
    "    # Generate response and store it in a variable\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        streamer=text_streamer,  # This streams output while generating\n",
    "        max_new_tokens=100,\n",
    "        use_cache=True,\n",
    "        temperature=0,\n",
    "        min_p=0,\n",
    "        top_k = 0\n",
    "    )\n",
    "    \n",
    "    \n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    print(\"\\nGenerated Output:\\n\", generated_text)\n",
    "    print('\\n')\n",
    "    print('\\n')\n",
    "\n",
    "# Example output\n",
    "\n",
    "    '''points Mised=  {\"pointsMissed\": [\"the product cost is 5\", \"its cost is 5\", \"just 5\"]}\n",
    " {\"pointsMissed\": [\"the cost of the product is 5\", \"the product cost is 5\", \"cost is 5 \", \"its cost is 5 \", \"just 5 \"]}<|im_end|>\n",
    "\n",
    "Generated Output:\n",
    " Act like an expert evaluator, where given an answer and a list of points to be covered in the answer you give a json of missing points\n",
    "Answer: Yeah. So the cost of product is $5. \n",
    "List of points to be covered: [['the cost of the product is 5', 'the product cost is 5', 'cost is 5 ', 'its cost is 5 ', 'just 5 ']] {\"pointsMissed\": [\"the cost of the product is 5\", \"the product cost is 5\", \"cost is 5 \", \"its cost is 5 \", \"just 5 \"]}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
