{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip install rouge-score nltk\n",
    "from unsloth.chat_templates import get_chat_template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = '/kaggle/input/train-and-test-dataset/formatted_training_data_v2.json'\n",
    "data = json.load(open(path))\n",
    "for i in data:\n",
    "    print(i)\n",
    "    print('\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finetuned\n",
    "from unsloth import FastLanguageModel\n",
    "import torch \n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True \n",
    "\n",
    "model,tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"/kaggle/input/qwen_2.5_v2/pytorch/default/1\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from transformers import TextStreamer\n",
    "import json\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Make sure we have NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Better normalization function\n",
    "def normalize_text(text):\n",
    "    \"\"\"Improved text normalization that preserves more semantic information.\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Convert to lowercase but preserve sentence structure\n",
    "    text = str(text).lower().strip()\n",
    "    # Replace multiple spaces but keep punctuation that affects meaning\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Use stemming for better word matching\n",
    "    words = nltk.word_tokenize(text)\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Improved JSON matching with semantic similarity\n",
    "def calculate_json_match(generated_json, reference_json, similarity_threshold=0.85):\n",
    "    \"\"\"Calculate JSON matching with semantic similarity.\"\"\"\n",
    "    try:\n",
    "        gen_points = generated_json.get('pointsMissed', [])\n",
    "        ref_points = reference_json.get('pointsMissed', [])\n",
    "        \n",
    "        if not gen_points or not ref_points:\n",
    "            return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "        \n",
    "        # Use sentence transformers for semantic matching\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "        # Embed all points\n",
    "        gen_embeddings = model.encode(gen_points)\n",
    "        ref_embeddings = model.encode(ref_points)\n",
    "        \n",
    "        # Calculate cosine similarity matrix\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        similarity_matrix = cosine_similarity(gen_embeddings, ref_embeddings)\n",
    "        \n",
    "        # Count matches using similarity threshold\n",
    "        matches = sum(1 for row in similarity_matrix if max(row) >= similarity_threshold)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = matches / len(gen_points) if gen_points else 0\n",
    "        recall = matches / len(ref_points) if ref_points else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {'precision': precision, 'recall': recall, 'f1': f1}\n",
    "    except Exception as e:\n",
    "        print(f\"Error in semantic matching: {e}\")\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "        \n",
    "def extract_json_from_text(text):\n",
    "    \"\"\"Extract JSON content from generated text, handling multi-line JSON structures.\"\"\"\n",
    "    print(\"TEXT\", text)\n",
    "    try:\n",
    "        # Find JSON pattern in the text (allowing for nested structures)\n",
    "        json_pattern = r'\\{[^{}]*\\{[^{}]*\\}[^{}]*\\}|\\{[^{}]*\\}'\n",
    "        matches = re.findall(json_pattern, text, re.DOTALL)\n",
    "        \n",
    "        if matches:\n",
    "            for match in matches[::-1]:  # Check from the last occurrence\n",
    "                try:\n",
    "                    # Remove extra characters and whitespace\n",
    "                    match = match.strip()\n",
    "                    if match.startswith(\"```json\"):\n",
    "                        match = match[7:].strip()\n",
    "                    if match.endswith(\"```\"):\n",
    "                        match = match[:-3].strip()\n",
    "                    return json.loads(match)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        return {}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting JSON: {e}\")\n",
    "        return {}\n",
    "\n",
    "def calculate_metrics(generated_text, reference_text):\n",
    "    \"\"\"Calculate BLEU and ROUGE scores between generated and reference texts.\"\"\"\n",
    "    try:\n",
    "        # Normalize texts\n",
    "        gen_normalized = normalize_text(generated_text)\n",
    "        ref_normalized = normalize_text(reference_text)\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        rouge_scores = scorer.score(ref_normalized, gen_normalized)\n",
    "        \n",
    "        # Prepare for BLEU calculation\n",
    "        reference_tokens = [nltk.word_tokenize(ref_normalized)]\n",
    "        generated_tokens = nltk.word_tokenize(gen_normalized)\n",
    "        \n",
    "        # Calculate BLEU with smoothing\n",
    "        smooth = SmoothingFunction()\n",
    "        bleu1 = sentence_bleu(reference_tokens, generated_tokens, \n",
    "                             weights=(1, 0, 0, 0),\n",
    "                             smoothing_function=smooth.method1)\n",
    "        \n",
    "        bleu4 = sentence_bleu(reference_tokens, generated_tokens, \n",
    "                             weights=(0.25, 0.25, 0.25, 0.25),\n",
    "                             smoothing_function=smooth.method1)\n",
    "        \n",
    "        return {\n",
    "            'bleu1': bleu1,\n",
    "            'bleu4': bleu4,\n",
    "            'rouge1': rouge_scores['rouge1'].fmeasure,\n",
    "            'rouge2': rouge_scores['rouge2'].fmeasure, \n",
    "            'rougeL': rouge_scores['rougeL'].fmeasure\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating metrics: {e}\")\n",
    "        return {\n",
    "            'bleu1': 0.0, 'bleu4': 0.0,\n",
    "            'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0\n",
    "        }\n",
    "\n",
    "def calculate_exact_match_rate(generated_json, reference_json):\n",
    "    \"\"\"Calculate exact match rate for JSON objects.\"\"\"\n",
    "    try:\n",
    "        if not generated_json or not reference_json:\n",
    "            print('Not generated')\n",
    "            return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "            \n",
    "        # Extract missing points arrays\n",
    "        gen_points = generated_json.get('pointsMissed', [])\n",
    "        ref_points = reference_json.get('pointsMissed', [])\n",
    "        \n",
    "        if not gen_points or not ref_points:\n",
    "            return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "            \n",
    "        # Normalize points\n",
    "        gen_points = [normalize_text(p) for p in gen_points]\n",
    "        ref_points = [normalize_text(p) for p in ref_points]\n",
    "        print(gen_points)\n",
    "        print(ref_points)\n",
    "        # Calculate how many points match exactly\n",
    "        matches = sum(1 for p in gen_points if p in ref_points)\n",
    "        \n",
    "        # Calculate precision: matches / generated points\n",
    "        precision = matches / len(gen_points) if gen_points else 0\n",
    "        \n",
    "        # Calculate recall: matches / reference points\n",
    "        recall = matches / len(ref_points) if ref_points else 0\n",
    "        \n",
    "        # F1 score\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating exact match: {e}\")\n",
    "        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "\n",
    "# Process evaluation data\n",
    "total_metrics = {\n",
    "    'bleu1': [], 'bleu4': [],\n",
    "    'rouge1': [], 'rouge2': [], 'rougeL': [],\n",
    "    'precision': [], 'recall': [], 'f1': []\n",
    "}\n",
    "\n",
    "# Run evaluation for each example\n",
    "for i in data:\n",
    "    try:\n",
    "        # Prepare the prompt\n",
    "        mes = 'Act like an expert evaluator, where given an answer and a list of points to be covered in the answer you give a json of missing points'\n",
    "        ans = mes + i['conversations'][1]['content']\n",
    "        reference_output = i['conversations'][2]['content']\n",
    "        message = ans\n",
    "        \n",
    "        print('\\nEvaluating example:')\n",
    "        print('Reference output:', reference_output)\n",
    "        \n",
    "        # Tokenize input and move to GPU\n",
    "        inputs = tokenizer(message, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        # Initialize the streamer\n",
    "        text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "        \n",
    "        # Generate response\n",
    "        output_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            streamer=text_streamer,  # This streams output while generating\n",
    "            max_new_tokens=300,\n",
    "            use_cache=True,\n",
    "            temperature=0.2,\n",
    "            min_p=0.2,\n",
    "            top_k=0\n",
    "        )\n",
    "        \n",
    "        # Decode the generated text\n",
    "        generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        # print(\"Generated Output:\", generated_text)\n",
    "        \n",
    "        # Extract JSON from both texts\n",
    "        reference_json = extract_json_from_text(reference_output)\n",
    "        generated_json = extract_json_from_text(generated_text)\n",
    "        print(\"Reference JSON:\", reference_json)\n",
    "        print(\"Generated JSON:\", generated_json)\n",
    "        \n",
    "        # Calculate text-level metrics\n",
    "        text_metrics = calculate_metrics(generated_text, reference_output)\n",
    "        print(\"Text Metrics:\", text_metrics)\n",
    "        \n",
    "        # Calculate JSON-level metrics\n",
    "        json_metrics = calculate_exact_match_rate(generated_json, reference_json)\n",
    "        print(\"JSON Metrics:\", json_metrics)\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        for key in text_metrics:\n",
    "            total_metrics[key].append(text_metrics[key])\n",
    "        for key in json_metrics:\n",
    "            total_metrics[key].append(json_metrics[key])\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example: {e}\")\n",
    "        continue\n",
    "\n",
    "# Calculate and display average metrics\n",
    "print(\"\\n===== EVALUATION RESULTS =====\")\n",
    "print(\"\\nAverage Metrics:\")\n",
    "for key in total_metrics:\n",
    "    if total_metrics[key]:\n",
    "        avg = np.mean(total_metrics[key])\n",
    "        print(f\"{key}: {avg:.4f}\")\n",
    "\n",
    "print(\"\\nNumber of examples evaluated:\", len(total_metrics['bleu1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
